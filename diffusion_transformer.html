<!doctype html>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Distill Template -->
<script src="https://distill.pub/template.v1.js"></script>

<!-- jQuery for loading header -->
<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

<!-- MathJax for rendering equations -->
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      tags: 'ams'
    },
    startup: {
      pageReady: () => {
        return MathJax.startup.defaultPageReady();
      }
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style>
    /* ── Global custom styles ── */
    .interactive-container {
        border: 1px solid rgba(0,0,0,0.1);
        border-radius: 8px;
        background: #f9f9f9;
        padding: 24px;
        margin: 2rem 0;
        display: flex;
        flex-direction: column;
        align-items: center;
        font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    }
    .interactive-container h4 {
        margin: 0 0 12px;
        font-size: 14px;
        text-transform: uppercase;
        letter-spacing: 1px;
        color: #888;
    }
    .canvas-wrapper {
        position: relative;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        background: white;
        border-radius: 4px;
        overflow: hidden;
    }
    .control-panel {
        margin-top: 14px;
        font-size: 0.9em;
        color: #555;
        text-align: center;
        max-width: 640px;
    }

    /* ── Derivation steps ── */
    .derivation-step {
        background: #f4f8fb;
        border-left: 4px solid #EBC043;
        padding: 16px 20px;
        margin: 1.2rem 0;
        border-radius: 0 6px 6px 0;
    }
    .derivation-step .step-label {
        font-weight: 700;
        color: #EBC043;
        font-size: 0.85em;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        margin-bottom: 6px;
    }

    /* ── Intuition callout boxes ── */
    .intuition-box {
        background: #fffbe6;
        border: 1px solid #EBC043;
        border-radius: 8px;
        padding: 16px 20px;
        margin: 1.5rem 0;
    }
    .intuition-box::before {
        content: '\1F4A1 Intuition';
        display: block;
        font-weight: 700;
        font-size: 0.85em;
        color: #b8941e;
        margin-bottom: 6px;
    }

    /* ── Buttons ── */
    button.train-btn {
        padding: 10px 20px;
        font-size: 14px;
        font-weight: bold;
        border: none;
        border-radius: 4px;
        cursor: pointer;
        transition: transform 0.1s, opacity 0.2s;
    }
    button.train-btn:active { transform: scale(0.96); }
    button.train-btn:disabled { background-color: #ccc !important; color: #888 !important; cursor: not-allowed; }
    .btn-play { background-color: #22c55e; color: white; }
    .btn-stop { background-color: #f97316; color: white; }
    .btn-reset { background-color: #e5e7eb; color: #374151; }
    .btn-primary { background-color: #f59e0b; color: white; }
    .btn-row {
        display: flex;
        gap: 10px;
        margin-top: 12px;
        align-items: center;
        flex-wrap: wrap;
        justify-content: center;
    }

    /* ── Slider rows ── */
    .slider-row {
        display: flex;
        align-items: center;
        gap: 12px;
        margin: 8px 0;
        flex-wrap: wrap;
        justify-content: center;
    }
    .slider-row label {
        font-weight: 600;
        min-width: 140px;
        text-align: right;
    }
    .slider-row input[type=range] {
        width: 240px;
        accent-color: #EBC043;
    }
    .slider-row .val {
        min-width: 60px;
        font-family: monospace;
    }

    /* ── Side-by-side panels ── */
    .side-panels {
        display: flex;
        gap: 20px;
        flex-wrap: wrap;
        justify-content: center;
        margin-top: 10px;
    }
    .side-panels .panel { text-align: center; }
    .side-panels .panel canvas { border: 1px solid #ccc; border-radius: 4px; }
    .side-panels .panel-label {
        font-weight: 600;
        margin-bottom: 8px;
        font-size: 0.95em;
    }

    /* ── Misc ── */
    .epoch-display {
        font-family: monospace;
        font-size: 14px;
        background: #eee;
        padding: 4px 12px;
        border-radius: 4px;
    }
    .arch-compare {
        display: flex;
        gap: 24px;
        flex-wrap: wrap;
        justify-content: center;
        margin: 10px 0;
    }
    .arch-card {
        background: white;
        border: 1px solid #e5e7eb;
        border-radius: 8px;
        padding: 16px;
        width: 260px;
        text-align: center;
    }
    .arch-card h4 {
        margin: 0 0 8px;
        font-size: 15px;
    }
    .arch-card canvas {
        border: 1px solid #eee;
        border-radius: 4px;
    }
</style>

<!-- Header Placeholder -->
<div id="includedContent"></div>
<script> 
    $(function(){
      $("#includedContent").load("header.html"); 
    });
</script> 

<dt-article>
  <h1>Diffusion Transformers (DiT)</h1>
  <h2>The Convergence of Architectures</h2>

  <hr>

  <!-- ============================================================
       SECTION 1 — Motivation
       ============================================================ -->
  <h2>1 &middot; Merging Two Worlds</h2>
  <p>
    Throughout this explainer we have traced two parallel lineages of generative modelling:
  </p>
  <ul>
    <li><strong>Diffusion models</strong> (with U-Net backbones) &mdash; excel at continuous, high-fidelity texture synthesis, but their locality bias limits global coherence, and their irregular architectures resist simple scaling.</li>
    <li><strong>Autoregressive Transformers</strong> &mdash; excel at capturing long-range dependencies and obey clean scaling laws (more compute &rarr; better quality), but they are slow (sequential token generation) and constrained by discrete vocabularies.</li>
  </ul>
  <p>
    The <strong>Diffusion Transformer (DiT)</strong>, introduced by Peebles &amp; Xie (2023), takes the best of both: it keeps the <em>diffusion training objective</em> (iteratively denoising a corrupted signal) but replaces the convolutional U-Net backbone with a <em>Vision Transformer</em> (ViT).
    State-of-the-art systems like <strong>Stable Diffusion 3</strong>, <strong>DALL·E 3</strong>, and <strong>Sora</strong> all adopt this paradigm.
  </p>

  <div class="intuition-box">
    Think of it this way: a U-Net is like reading a book through a magnifying glass &mdash; you see nearby words clearly but must scroll to understand the whole page.
    A Transformer reads the entire page at once, so every word knows the context of every other word from the start.
  </div>

  <!-- ============================================================
       SECTION 2 — Architecture Overview
       ============================================================ -->
  <h2>2 &middot; Architecture Overview</h2>
  <p>
    At the highest level, a DiT receives three inputs and produces one output:
  </p>
  <ol>
    <li><strong>Noisy latent</strong> $z_t$ &mdash; the image in latent space at timestep $t$.</li>
    <li><strong>Timestep embedding</strong> $t$ &mdash; tells the network how much noise is present.</li>
    <li><strong>Conditioning signal</strong> $c$ &mdash; a class label, text embedding, or other guidance.</li>
  </ol>
  <p>The output is the predicted noise $\hat\epsilon_\theta(z_t, t, c)$ (or equivalently the predicted velocity $v_\theta$). The pipeline is:</p>

  <div class="derivation-step">
    <div class="step-label">DiT Forward Pass</div>
    $$
    z_t \;\xrightarrow{\text{Patchify + Embed}}\; \mathbf{X}^{(0)}
    \;\xrightarrow{\text{DiT Block } \times N}\; \mathbf{X}^{(N)}
    \;\xrightarrow{\text{Linear + Unpatchify}}\; \hat\epsilon_\theta
    $$
  </div>

  <p>Each stage is explored in detail below.</p>

  <!-- ============================================================
       SECTION 3 — Patchify & Positional Encoding
       ============================================================ -->
  <h2>3 &middot; Patchify &amp; Positional Encoding</h2>
  <p>
    Transformers process <em>sequences</em> of tokens. To feed a 2-D latent image into a Transformer we must convert it into a 1-D token sequence. DiT borrows the <strong>patchification</strong> strategy from Vision Transformers (ViT):
  </p>
  <ol>
    <li><strong>Divide</strong> the $H \times W \times C$ latent into a grid of non-overlapping $p \times p$ patches.</li>
    <li><strong>Flatten</strong> each patch into a vector of dimension $d_{\text{patch}} = p^2 \cdot C$.</li>
    <li><strong>Linearly project</strong> each flattened patch into the Transformer's hidden dimension $D$.</li>
    <li><strong>Add positional encodings</strong> so the model knows <em>where</em> each patch came from.</li>
  </ol>

  <div class="derivation-step">
    <div class="step-label">Patchify Math</div>
    Given a latent $z \in \mathbb{R}^{H \times W \times C}$ and patch size $p$, the number of tokens is:
    $$
    T = \frac{H}{p} \cdot \frac{W}{p}
    $$
    Each token $\mathbf{x}_i$ is obtained via a learnable linear projection $\mathbf{E} \in \mathbb{R}^{D \times d_{\text{patch}}}$:
    $$
    \mathbf{x}_i = \mathbf{E}\,\text{flatten}(\text{patch}_i) + \mathbf{p}_i, \qquad i = 1, \dots, T
    $$
    where $\mathbf{p}_i$ is a learnable (or sinusoidal) positional embedding for position $i$.
  </div>

  <div class="intuition-box">
    With patch size $p = 2$ on a $32 \times 32$ latent, we get $T = 256$ tokens &mdash; comparable to a short sentence.
    A $p = 4$ patch size gives only 64 tokens, which is much cheaper but captures less spatial detail.
    The patch size is a core resolution/cost trade-off.
  </div>

  <h3>Interactive: Patchify Visualiser</h3>
  <p>
    Drag the <strong>Patch Size</strong> slider to see how the latent image is divided into tokens. Watch how the token count changes. Smaller patches mean more tokens and finer spatial detail, but quadratically more attention cost.
  </p>

  <div class="interactive-container">
    <h4>Patchify &amp; Token Count</h4>
    <div class="canvas-wrapper">
      <canvas id="patchifyCanvas" width="320" height="320"></canvas>
    </div>
    <div class="slider-row">
      <label for="patchSizeSlider">Patch Size $p$:</label>
      <input type="range" id="patchSizeSlider" min="1" max="5" step="1" value="2">
      <span class="val" id="patchSizeVal">2</span>
    </div>
    <div class="epoch-display" id="tokenCountDisplay">Tokens: 256 &nbsp;|&nbsp; Attention cost ∝ 256² = 65 536</div>
  </div>

  <!-- ============================================================
       SECTION 4 — Self-Attention Recap
       ============================================================ -->
  <h2>4 &middot; Self-Attention for Images</h2>
  <p>
    Once we have tokens, we apply multi-head self-attention. Each head computes:
  </p>

  <div class="derivation-step">
    <div class="step-label">Scaled Dot-Product Attention</div>
    $$
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
      = \text{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V}
    $$
    where $\mathbf{Q} = \mathbf{X}\mathbf{W}_Q$, $\mathbf{K} = \mathbf{X}\mathbf{W}_K$, $\mathbf{V} = \mathbf{X}\mathbf{W}_V$ and $d_k$ is the head dimension.
  </div>

  <p>
    Crucially, the attention matrix $\mathbf{A} \in \mathbb{R}^{T \times T}$ means <strong>every patch can attend to every other patch</strong>.
    This is fundamentally different from a convolution, which only looks at a local $3 \times 3$ or $5 \times 5$ neighbourhood.
    Global attention allows the model to correlate distant image regions from the very first layer &mdash;
    e.g., understanding that a shadow on the ground should match the pose of the person casting it.
  </p>

  <h3>Interactive: Global Attention Map</h3>
  <p>
    Hover over any patch below. The <span style="color:#7c3aed; font-weight:bold;">purple lines</span> show which other patches it attends to (thicker = higher weight).
    Notice how semantically related patches (same colour region) attend more strongly to each other,
    but there is always some attention to distant patches &mdash; global context.
  </p>

  <div class="interactive-container">
    <h4>Self-Attention Visualisation</h4>
    <div class="canvas-wrapper">
      <canvas id="attnCanvas" width="360" height="360"></canvas>
    </div>
    <div class="control-panel" id="attnStatus">Hover over a patch to see its attention field.</div>
  </div>

  <!-- ============================================================
       SECTION 5 — DiT Block & adaLN-Zero
       ============================================================ -->
  <h2>5 &middot; The DiT Block &amp; adaLN-Zero</h2>
  <p>
    A standard Transformer block consists of <em>Layer Norm → Multi-Head Self-Attention → Layer Norm → Feed-Forward Network (FFN)</em>.
    The key question for DiT is: <strong>how do we inject the conditioning signal</strong> (timestep $t$ and class/text $c$)?
  </p>
  <p>
    Peebles &amp; Xie explored four conditioning strategies and found that <strong>adaptive Layer Norm with a zero-initialised gate</strong> (adaLN-Zero) works best.
    Instead of fixed Layer Norm parameters $(\gamma, \beta)$, these are <em>predicted from the condition</em>:
  </p>

  <div class="derivation-step">
    <div class="step-label">adaLN-Zero Mechanism</div>
    Given the conditioning vector $\mathbf{c} = \text{MLP}(\text{embed}(t) + \text{embed}(y))$, we regress six modulation parameters per block:
    $$
    (\gamma_1, \beta_1, \alpha_1, \gamma_2, \beta_2, \alpha_2) = \text{Linear}(\mathbf{c})
    $$
    The modulated forward pass becomes:
    \begin{align}
    \mathbf{h} &= \mathbf{X} + \alpha_1 \odot \text{MHSA}\!\big(\gamma_1 \odot \text{LN}(\mathbf{X}) + \beta_1\big) \tag{attention arm} \\
    \mathbf{X}' &= \mathbf{h} + \alpha_2 \odot \text{FFN}\!\big(\gamma_2 \odot \text{LN}(\mathbf{h}) + \beta_2\big) \tag{FFN arm}
    \end{align}
    The gate parameters $(\alpha_1, \alpha_2)$ are initialised to <strong>zero</strong>, so at the start of training each DiT block behaves as an identity function.
  </div>

  <div class="intuition-box">
    Why zero-initialise the gates? It makes training more stable.
    At initialisation every DiT block is a no-op, so the network outputs pure noise &mdash; matching the prior.
    The model then gradually "turns on" each block as it learns useful transformations, instead of disrupting the signal with random large updates.
  </div>

  <h3>Interactive: adaLN-Zero Conditioning</h3>
  <p>
    The demo below shows how different conditioning signals modulate the same input.
    Adjust the <strong>Timestep</strong> and <strong>Class</strong> sliders to see how the scale ($\gamma$) and shift ($\beta$) parameters change, and how the gate ($\alpha$) gradually opens during training.
  </p>

  <div class="interactive-container">
    <h4>adaLN-Zero Modulation</h4>
    <div class="side-panels">
      <div class="panel">
        <div class="panel-label">Input Tokens</div>
        <canvas id="adalnInput" width="180" height="180"></canvas>
      </div>
      <div class="panel">
        <div class="panel-label">After Modulation</div>
        <canvas id="adalnOutput" width="180" height="180"></canvas>
      </div>
      <div class="panel">
        <div class="panel-label">Modulation Params</div>
        <canvas id="adalnParams" width="180" height="180"></canvas>
      </div>
    </div>
    <div class="slider-row">
      <label for="adalnTimestep">Timestep $t$:</label>
      <input type="range" id="adalnTimestep" min="0" max="100" value="80">
      <span class="val" id="adalnTVal">80</span>
    </div>
    <div class="slider-row">
      <label for="adalnClass">Class label:</label>
      <input type="range" id="adalnClass" min="0" max="4" step="1" value="0">
      <span class="val" id="adalnCVal">Landscape</span>
    </div>
    <div class="slider-row">
      <label for="adalnGate">Gate $\alpha$ (training):</label>
      <input type="range" id="adalnGate" min="0" max="100" value="0">
      <span class="val" id="adalnGVal">0.00</span>
    </div>
  </div>

  <!-- ============================================================
       SECTION 6 — Training Objective
       ============================================================ -->
  <h2>6 &middot; Training Objective</h2>
  <p>
    DiT operates in <em>latent space</em> (just like the Latent Diffusion models from the previous section).
    A pre-trained VAE encoder maps an image $x$ to a latent $z_0 = \mathcal{E}(x)$.
    The forward diffusion process adds Gaussian noise:
  </p>

  <div class="derivation-step">
    <div class="step-label">Forward Process</div>
    $$
    z_t = \sqrt{\bar\alpha_t}\,z_0 + \sqrt{1 - \bar\alpha_t}\,\epsilon, \qquad \epsilon \sim \mathcal{N}(0, \mathbf{I})
    $$
    where $\bar\alpha_t = \prod_{s=1}^{t} (1 - \beta_s)$ is the cumulative noise schedule.
  </div>

  <p>The DiT is trained to predict the added noise:</p>

  <div class="derivation-step">
    <div class="step-label">DiT Loss</div>
    $$
    \mathcal{L}_{\text{DiT}} = \mathbb{E}_{z_0, \epsilon, t, c}\!\left[\,\|\epsilon - \hat\epsilon_\theta(z_t, t, c)\|^2\,\right]
    $$
    This is identical to the standard denoising score-matching objective, but the denoiser $\hat\epsilon_\theta$ is now a <strong>Transformer</strong> instead of a U-Net.
  </div>

  <div class="intuition-box">
    The loss function doesn't change &mdash; only the architecture of the denoiser.
    This means all the theory about diffusion (noise schedules, ELBO bounds, score matching) carries over unchanged.
    The improvement comes entirely from the Transformer's better inductive biases for global reasoning and its clean scaling behaviour.
  </div>

  <!-- ============================================================
       SECTION 7 — Classifier-Free Guidance
       ============================================================ -->
  <h2>7 &middot; Classifier-Free Guidance</h2>
  <p>
    To steer generation towards a desired condition $c$, DiT uses <strong>classifier-free guidance (CFG)</strong>.
    During training, the condition $c$ is randomly dropped (replaced with a null token $\varnothing$) some fraction of the time.
    At inference, the model computes both the conditional and unconditional predictions and interpolates:
  </p>

  <div class="derivation-step">
    <div class="step-label">CFG Equation</div>
    $$
    \tilde\epsilon_\theta(z_t, t, c) = (1 + w)\,\hat\epsilon_\theta(z_t, t, c) - w\,\hat\epsilon_\theta(z_t, t, \varnothing)
    $$
    where $w$ is the guidance scale. When $w = 0$ we recover the conditional model; as $w$ increases, the output is pushed further toward the condition.
  </div>

  <h3>Interactive: Classifier-Free Guidance Strength</h3>
  <p>
    Drag the guidance scale $w$ to see how it changes the generated output.
    Low $w$ gives diverse but sometimes off-topic results; high $w$ gives sharp, on-topic images but may oversaturate.
  </p>

  <div class="interactive-container">
    <h4>Classifier-Free Guidance</h4>
    <div class="side-panels">
      <div class="panel">
        <div class="panel-label">Unconditional $\hat\epsilon(\varnothing)$</div>
        <canvas id="cfgUncond" width="160" height="160"></canvas>
      </div>
      <div class="panel">
        <div class="panel-label">Conditional $\hat\epsilon(c)$</div>
        <canvas id="cfgCond" width="160" height="160"></canvas>
      </div>
      <div class="panel">
        <div class="panel-label">Guided $\tilde\epsilon$</div>
        <canvas id="cfgGuided" width="160" height="160"></canvas>
      </div>
    </div>
    <div class="slider-row">
      <label for="cfgScale">Guidance $w$:</label>
      <input type="range" id="cfgScale" min="0" max="200" value="75">
      <span class="val" id="cfgScaleVal">7.5</span>
    </div>
    <div class="epoch-display" id="cfgInfo">w = 0 → unconditional &nbsp;|&nbsp; w → ∞ → oversaturated</div>
  </div>

  <!-- ============================================================
       SECTION 8 — U-Net vs DiT
       ============================================================ -->
  <h2>8 &middot; U-Net vs. DiT: Architecture Comparison</h2>
  <p>
    To appreciate what DiT changes, let us compare the two backbones side by side:
  </p>
  <table style="width:100%; border-collapse:collapse; margin:1rem 0; font-size:0.95em;">
    <thead>
      <tr style="background:#f4f4f4;">
        <th style="padding:8px; border:1px solid #ddd; text-align:left;">Property</th>
        <th style="padding:8px; border:1px solid #ddd; text-align:center;">U-Net</th>
        <th style="padding:8px; border:1px solid #ddd; text-align:center;">DiT</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="padding:8px; border:1px solid #ddd;">Core operation</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Conv + Skip connections</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Self-Attention + FFN</td>
      </tr>
      <tr>
        <td style="padding:8px; border:1px solid #ddd;">Receptive field</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Local (grows with depth)</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Global (every layer)</td>
      </tr>
      <tr>
        <td style="padding:8px; border:1px solid #ddd;">Conditioning</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Cross-attention layers</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">adaLN-Zero</td>
      </tr>
      <tr>
        <td style="padding:8px; border:1px solid #ddd;">Scale-up recipe</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Ad-hoc (width/depth/attention mix)</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Clean (add more Transformer blocks)</td>
      </tr>
      <tr>
        <td style="padding:8px; border:1px solid #ddd;">Scaling behaviour</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Diminishing returns</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Predictable log-linear improvement</td>
      </tr>
      <tr>
        <td style="padding:8px; border:1px solid #ddd;">FLOPs at inference</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Lower (locality)</td>
        <td style="padding:8px; border:1px solid #ddd; text-align:center;">Higher ($O(T^2)$ attention)</td>
      </tr>
    </tbody>
  </table>

  <h3>Interactive: Architecture Comparison</h3>
  <p>
    Watch how information flows differently in a U-Net (local convolutions, growing receptive field) versus a DiT (instant global attention). Press <strong>Step</strong> to advance each network by one layer.
  </p>

  <div class="interactive-container">
    <h4>U-Net vs DiT &mdash; Information Flow</h4>
    <div class="arch-compare">
      <div class="arch-card">
        <h4>U-Net (Conv)</h4>
        <canvas id="unetCanvas" width="220" height="220"></canvas>
      </div>
      <div class="arch-card">
        <h4>DiT (Attention)</h4>
        <canvas id="ditCompCanvas" width="220" height="220"></canvas>
      </div>
    </div>
    <div class="btn-row">
      <button class="train-btn btn-play" id="archStepBtn">▶ Step Layer</button>
      <button class="train-btn btn-reset" id="archResetBtn">↺ Reset</button>
      <span class="epoch-display" id="archLayerDisplay">Layer: 0 / 6</span>
    </div>
  </div>

  <!-- ============================================================
       SECTION 9 — Scaling Laws
       ============================================================ -->
  <h2>9 &middot; Scaling Laws</h2>
  <p>
    One of the most important results from the DiT paper is that Transformer-based diffusion models obey <strong>scaling laws</strong> analogous to those observed in large language models.
    Specifically, as the model size and training compute increase, the FID (Fréchet Inception Distance) decreases <em>predictably</em>:
  </p>

  <div class="derivation-step">
    <div class="step-label">Scaling Law (Empirical)</div>
    $$
    \text{FID} \propto \left(\frac{C_0}{C}\right)^{\alpha}
    $$
    where $C$ is the total training compute in GFLOPs, and $\alpha$ is the scaling exponent (roughly $0.3\text{–}0.5$ for image generation tasks). The DiT paper shows four model sizes all falling on a single scaling curve:
    <br><br>
    <strong>DiT-S</strong> (33M) &rarr; <strong>DiT-B</strong> (130M) &rarr; <strong>DiT-L</strong> (458M) &rarr; <strong>DiT-XL</strong> (675M)
  </div>

  <div class="intuition-box">
    Scaling laws mean we can <em>predict</em> how good a larger model will be before training it. This is enormously valuable: it lets researchers allocate compute budgets rationally and gives confidence that simply making the model bigger will yield better images &mdash; no architectural re-engineering needed.
  </div>

  <h3>Interactive: Scaling Curve</h3>
  <p>
    The chart below plots FID (lower is better) against model GFLOPs. Drag the <strong>Model Scale</strong> slider to see where different model sizes land on the scaling curve.
  </p>

  <div class="interactive-container">
    <h4>DiT Scaling Law</h4>
    <div class="canvas-wrapper">
      <canvas id="scalingCanvas" width="520" height="280"></canvas>
    </div>
    <div class="slider-row">
      <label for="scaleSlider">Model GFLOPs:</label>
      <input type="range" id="scaleSlider" min="1" max="120" value="20">
      <span class="val" id="scaleVal">20</span>
    </div>
    <div class="epoch-display" id="scaleFidDisplay">Predicted FID: —</div>
  </div>

  <!-- ============================================================
       SECTION 10 — Full Denoising Demo
       ============================================================ -->
  <h2>10 &middot; Interactive: Full Denoising Process</h2>
  <p>
    Below is a complete simulation of the DiT denoising loop. Press <strong>Start</strong> to watch the noisy latent progressively resolve into a clean image over many timesteps. Each step, the DiT predicts the noise, subtracts a portion, and passes the result to the next step.
  </p>

  <div class="interactive-container">
    <h4>DiT Denoising Loop</h4>
    <div class="side-panels">
      <div class="panel">
        <div class="panel-label">Current $z_t$</div>
        <canvas id="denoiseLatent" width="200" height="200"></canvas>
      </div>
      <div class="panel">
        <div class="panel-label">Predicted Noise $\hat\epsilon$</div>
        <canvas id="denoisePred" width="200" height="200"></canvas>
      </div>
      <div class="panel">
        <div class="panel-label">Decoded Image</div>
        <canvas id="denoiseImage" width="200" height="200"></canvas>
      </div>
    </div>
    <div class="btn-row">
      <button class="train-btn btn-play" id="denoiseStartBtn">▶ Start Denoising</button>
      <button class="train-btn btn-reset" id="denoiseResetBtn">↺ Reset to Noise</button>
      <span class="epoch-display" id="denoiseStep">Step: 0 / 50</span>
    </div>
    <div class="slider-row">
      <label for="denoiseSpeed">Speed:</label>
      <input type="range" id="denoiseSpeed" min="1" max="10" value="5">
      <span class="val" id="denoiseSpeedVal">5</span>
    </div>
  </div>

  <!-- ============================================================
       SECTION 11 — Strengths & Limitations
       ============================================================ -->
  <h2>11 &middot; Strengths &amp; Limitations</h2>
  <h3>Strengths</h3>
  <ul>
    <li><strong>Clean scaling</strong> &mdash; More parameters and compute reliably improve quality (FID follows a power law).</li>
    <li><strong>Global coherence</strong> &mdash; Self-attention across all patches enables long-range spatial consistency.</li>
    <li><strong>Unified architecture</strong> &mdash; The same Transformer block is used everywhere; no hand-crafted downsampling/upsampling stages.</li>
    <li><strong>Multi-modal conditioning</strong> &mdash; adaLN-Zero seamlessly integrates text, class labels, and other signals.</li>
    <li><strong>Extensible to video</strong> &mdash; Temporal attention blocks can be interleaved for video generation (as in Sora).</li>
  </ul>

  <h3>Limitations</h3>
  <ul>
    <li><strong>Quadratic attention cost</strong> &mdash; Self-attention is $O(T^2)$, limiting resolution or requiring patch size trade-offs.</li>
    <li><strong>Still iterative</strong> &mdash; Like all diffusion models, generation requires many forward passes (though distillation can reduce this).</li>
    <li><strong>Memory hungry</strong> &mdash; Large DiT models require significant GPU memory for training and inference.</li>
    <li><strong>Less inductive bias</strong> &mdash; Without convolution's translation equivariance, DiT may need more data to learn basic visual priors.</li>
  </ul>

  <!-- ============================================================
       SECTION 12 — Summary
       ============================================================ -->
  <h2>12 &middot; Summary &amp; The Full Pipeline</h2>
  <p>
    The Diffusion Transformer represents the <strong>convergence point</strong> of the two main generative modelling paradigms we have studied.
    By combining the iterative denoising process of diffusion with the global attention and clean scaling of Transformers, DiT achieves state-of-the-art image quality and forms the backbone of the most capable generative AI systems.
  </p>

  <div class="derivation-step">
    <div class="step-label">The Complete Modern Pipeline</div>
    $$
    x \;\xrightarrow{\mathcal{E}}\; z_0
    \;\xrightarrow{\text{noise}}\; z_T
    \;\xrightarrow[\text{DiT blocks} \times N]{\text{denoise}}\; \hat{z}_0
    \;\xrightarrow{\mathcal{D}}\; \hat{x}
    $$
    <ol style="margin-top:8px;">
      <li>A <strong>VAE encoder</strong> $\mathcal{E}$ compresses the image to a latent.</li>
      <li><strong>Forward diffusion</strong> corrupts the latent with Gaussian noise.</li>
      <li>A <strong>Diffusion Transformer</strong> iteratively denoises, conditioned on text/class via adaLN-Zero.</li>
      <li>A <strong>VAE decoder</strong> $\mathcal{D}$ reconstructs the final image from the clean latent.</li>
    </ol>
  </div>

  <p>
    Each section of this explainer has built upon the last: from the basics of VAEs and GANs, through pixel-space and latent-space diffusion, to autoregressive models, and finally here to Diffusion Transformers. Together they form the full story of modern generative image AI.
  </p>

</dt-article>

<script>
/* ================================================================
   DIFFUSION TRANSFORMER — INTERACTIVE DEMOS
   ================================================================ */

/* ----------------------------------------------------------------
   HELPER: simple seeded pseudo-random for reproducible visuals
   ---------------------------------------------------------------- */
function mulberry32(a){return function(){a|=0;a=a+0x6D2B79F5|0;var t=Math.imul(a^a>>>15,1|a);t=t+Math.imul(t^t>>>7,61|t)^t;return((t^t>>>14)>>>0)/4294967296;}}

/* ----------------------------------------------------------------
   DEMO 1 — PATCHIFY VISUALISER
   ---------------------------------------------------------------- */
(function(){
  const cv = document.getElementById('patchifyCanvas');
  const cx = cv.getContext('2d');
  const W = cv.width, H = cv.height;
  const slider = document.getElementById('patchSizeSlider');
  const valSpan = document.getElementById('patchSizeVal');
  const tokenDisp = document.getElementById('tokenCountDisplay');

  // Fake latent image: colourful gradient
  const imgData = cx.createImageData(W, H);
  const rng = mulberry32(42);
  for(let y=0;y<H;y++) for(let x=0;x<W;x++){
    const i=(y*W+x)*4;
    imgData.data[i]  = Math.floor(120+80*Math.sin(x*0.04)+40*rng());
    imgData.data[i+1]= Math.floor(140+60*Math.cos(y*0.05)+30*rng());
    imgData.data[i+2]= Math.floor(180+50*Math.sin((x+y)*0.03)+30*rng());
    imgData.data[i+3]= 255;
  }

  function draw(){
    cx.putImageData(imgData, 0, 0);
    const pIdx = parseInt(slider.value);
    const pMap = [0, 2, 4, 8, 16, 32];
    const p = pMap[pIdx];
    valSpan.textContent = p;
    // latent is conceptually 32x32; map patch size to pixel grid
    const gridW = 32 / p;
    const gridH = 32 / p;
    const T = gridW * gridH;
    const cellW = W / gridW;
    const cellH = H / gridH;

    cx.strokeStyle = 'rgba(235,192,67,0.85)';
    cx.lineWidth = 2;
    for(let r=0;r<gridH;r++) for(let c=0;c<gridW;c++){
      cx.strokeRect(c*cellW, r*cellH, cellW, cellH);
    }
    // label
    cx.fillStyle = 'rgba(0,0,0,0.7)';
    cx.font = 'bold 13px monospace';
    cx.fillText(gridW+'×'+gridH+' = '+T+' tokens', 8, 18);

    const cost = T*T;
    tokenDisp.textContent = 'Tokens: '+T+' | Attention cost ∝ '+T+'² = '+cost.toLocaleString();
  }

  slider.addEventListener('input', draw);
  draw();
})();


/* ----------------------------------------------------------------
   DEMO 2 — GLOBAL ATTENTION MAP
   ---------------------------------------------------------------- */
(function(){
  const cv = document.getElementById('attnCanvas');
  const cx = cv.getContext('2d');
  const W = cv.width, H = cv.height;
  const G = 12; // 12×12 grid
  const cell = W / G;
  const status = document.getElementById('attnStatus');

  // Target colour for each patch (landscape)
  function targetCol(r,c){
    if(r<5) return [135,195,235];
    if(r<5 && c>8) return [255,215,0];
    if(r===5) return [100,180,100];
    return [34,139,60];
  }

  const rng = mulberry32(7);
  // precompute soft-attention weights: similar colour → high weight, else random low
  function attWeight(r1,c1,r2,c2){
    const a=targetCol(r1,c1), b=targetCol(r2,c2);
    const diff = Math.abs(a[0]-b[0])+Math.abs(a[1]-b[1])+Math.abs(a[2]-b[2]);
    if(diff<80) return 0.5 + rng()*0.5;
    return rng()*0.15;
  }

  function drawBase(){
    cx.clearRect(0,0,W,H);
    for(let r=0;r<G;r++) for(let c=0;c<G;c++){
      const col=targetCol(r,c);
      cx.fillStyle=`rgb(${col[0]},${col[1]},${col[2]})`;
      cx.fillRect(c*cell, r*cell, cell, cell);
      cx.strokeStyle='rgba(0,0,0,0.08)';
      cx.strokeRect(c*cell, r*cell, cell, cell);
    }
  }

  function drawAttn(hr, hc){
    drawBase();
    const ox = hc*cell+cell/2, oy = hr*cell+cell/2;
    // highlight origin
    cx.strokeStyle='#fff'; cx.lineWidth=3;
    cx.strokeRect(hc*cell, hr*cell, cell, cell);
    cx.lineWidth=1;

    for(let r=0;r<G;r++) for(let c=0;c<G;c++){
      if(r===hr && c===hc) continue;
      const w = attWeight(hr,hc,r,c);
      if(w<0.1) continue;
      const tx = c*cell+cell/2, ty = r*cell+cell/2;
      cx.beginPath();
      cx.moveTo(ox,oy); cx.lineTo(tx,ty);
      cx.strokeStyle=`rgba(124,58,237,${w*0.7})`;
      cx.lineWidth = w*3;
      cx.stroke();
    }
    status.textContent = `Token [${hr},${hc}] attending — thicker lines = stronger attention weight.`;
  }

  cv.addEventListener('mousemove', e=>{
    const rect=cv.getBoundingClientRect();
    const c=Math.floor((e.clientX-rect.left)/cell);
    const r=Math.floor((e.clientY-rect.top)/cell);
    if(r>=0&&r<G&&c>=0&&c<G) drawAttn(r,c);
  });
  cv.addEventListener('mouseleave', ()=>{ drawBase(); status.textContent='Hover over a patch to see its attention field.'; });
  drawBase();
})();


/* ----------------------------------------------------------------
   DEMO 3 — adaLN-Zero CONDITIONING
   ---------------------------------------------------------------- */
(function(){
  const cvIn  = document.getElementById('adalnInput');
  const cvOut = document.getElementById('adalnOutput');
  const cvPar = document.getElementById('adalnParams');
  const cxIn  = cvIn.getContext('2d');
  const cxOut = cvOut.getContext('2d');
  const cxPar = cvPar.getContext('2d');
  const sT = document.getElementById('adalnTimestep');
  const sC = document.getElementById('adalnClass');
  const sG = document.getElementById('adalnGate');
  const vT = document.getElementById('adalnTVal');
  const vC = document.getElementById('adalnCVal');
  const vG = document.getElementById('adalnGVal');

  const classNames = ['Landscape','Portrait','Abstract','Animal','Building'];
  const classHues  = [120, 0, 270, 30, 200]; // HSL hues for each class

  // Fixed "input tokens" pattern
  const rng = mulberry32(99);
  const SZ = cvIn.width;
  const grid = 6;
  const cellSz = SZ / grid;
  const inputVals = [];
  for(let i=0;i<grid*grid;i++) inputVals.push(rng());

  function drawInput(){
    for(let r=0;r<grid;r++) for(let c=0;c<grid;c++){
      const v = inputVals[r*grid+c];
      const g = Math.floor(v*255);
      cxIn.fillStyle=`rgb(${g},${g},${g})`;
      cxIn.fillRect(c*cellSz, r*cellSz, cellSz, cellSz);
      cxIn.strokeStyle='rgba(0,0,0,0.1)';
      cxIn.strokeRect(c*cellSz, r*cellSz, cellSz, cellSz);
    }
  }

  function update(){
    const t = parseInt(sT.value)/100;       // 0..1
    const cls = parseInt(sC.value);
    const gate = parseInt(sG.value)/100;    // 0..1

    vT.textContent = sT.value;
    vC.textContent = classNames[cls];
    vG.textContent = gate.toFixed(2);

    // Compute modulation params based on t and class
    const hue = classHues[cls];
    const gamma = 0.5 + t * 1.0;  // scale grows with noise
    const beta  = (1 - t) * 0.3;  // shift decreases with noise
    const alpha = gate;

    // Draw modulated output
    for(let r=0;r<grid;r++) for(let c=0;c<grid;c++){
      const v = inputVals[r*grid+c];
      const modulated = Math.max(0, Math.min(1, (v * gamma + beta) * alpha + v * (1 - alpha)));
      // Colour by class hue
      const sat = 40 + modulated * 60;
      const lum = 30 + modulated * 50;
      cxOut.fillStyle = `hsl(${hue}, ${sat}%, ${lum}%)`;
      cxOut.fillRect(c*cellSz, r*cellSz, cellSz, cellSz);
      cxOut.strokeStyle='rgba(0,0,0,0.1)';
      cxOut.strokeRect(c*cellSz, r*cellSz, cellSz, cellSz);
    }

    // Draw parameter visualisation
    cxPar.clearRect(0, 0, SZ, SZ);
    cxPar.fillStyle = '#f4f8fb';
    cxPar.fillRect(0, 0, SZ, SZ);

    const labels = ['γ (scale)', 'β (shift)', 'α (gate)'];
    const vals   = [gamma, beta, alpha];
    const colors = ['#3b82f6', '#22c55e', '#EBC043'];
    const barW = 40;
    const gap = (SZ - 3 * barW) / 4;

    for(let i=0;i<3;i++){
      const x = gap + i * (barW + gap);
      const maxH = SZ - 50;
      const h = Math.min(vals[i], 2) / 2 * maxH;
      cxPar.fillStyle = colors[i];
      cxPar.fillRect(x, SZ - 25 - h, barW, h);
      cxPar.strokeStyle = 'rgba(0,0,0,0.2)';
      cxPar.strokeRect(x, SZ - 25 - h, barW, h);
      cxPar.fillStyle = '#333';
      cxPar.font = '11px sans-serif';
      cxPar.textAlign = 'center';
      cxPar.fillText(labels[i], x + barW/2, SZ - 8);
      cxPar.fillText(vals[i].toFixed(2), x + barW/2, SZ - 30 - h);
    }
  }

  sT.addEventListener('input', update);
  sC.addEventListener('input', update);
  sG.addEventListener('input', update);
  drawInput();
  update();
})();


/* ----------------------------------------------------------------
   DEMO 4 — CLASSIFIER-FREE GUIDANCE
   ---------------------------------------------------------------- */
(function(){
  const cvU = document.getElementById('cfgUncond');
  const cvC = document.getElementById('cfgCond');
  const cvG = document.getElementById('cfgGuided');
  const cxU = cvU.getContext('2d');
  const cxC = cvC.getContext('2d');
  const cxG = cvG.getContext('2d');
  const slider = document.getElementById('cfgScale');
  const valSpan = document.getElementById('cfgScaleVal');
  const info = document.getElementById('cfgInfo');

  const SZ = cvU.width;
  const rng1 = mulberry32(55);
  const rng2 = mulberry32(77);

  // Precompute "unconditional" and "conditional" noise fields
  const uncond = [], cond = [];
  for(let y=0;y<SZ;y++) for(let x=0;x<SZ;x++){
    // Unconditional: random noise-ish landscape
    uncond.push([
      120 + rng1()*60,
      120 + rng1()*60,
      120 + rng1()*60
    ]);
    // Conditional: biased toward blue sky / green ground
    const isTop = y < SZ * 0.5;
    if(isTop){
      cond.push([100+rng2()*40, 160+rng2()*50, 210+rng2()*30]);
    } else {
      cond.push([30+rng2()*40, 120+rng2()*60, 30+rng2()*30]);
    }
  }

  function drawField(ctx, data){
    const img = ctx.createImageData(SZ, SZ);
    for(let i=0;i<data.length;i++){
      img.data[i*4]   = Math.max(0,Math.min(255,data[i][0]));
      img.data[i*4+1] = Math.max(0,Math.min(255,data[i][1]));
      img.data[i*4+2] = Math.max(0,Math.min(255,data[i][2]));
      img.data[i*4+3] = 255;
    }
    ctx.putImageData(img, 0, 0);
  }

  function update(){
    const w = parseInt(slider.value) / 10; // 0..20
    valSpan.textContent = w.toFixed(1);

    drawField(cxU, uncond);
    drawField(cxC, cond);

    // Guided = (1+w)*cond - w*uncond
    const guided = [];
    for(let i=0;i<uncond.length;i++){
      guided.push([
        (1+w)*cond[i][0] - w*uncond[i][0],
        (1+w)*cond[i][1] - w*uncond[i][1],
        (1+w)*cond[i][2] - w*uncond[i][2]
      ]);
    }
    drawField(cxG, guided);

    if(w < 1) info.textContent = 'Low guidance — diverse but unfocused.';
    else if(w < 5) info.textContent = 'Moderate guidance — good balance of quality and diversity.';
    else if(w < 12) info.textContent = 'High guidance — sharp and on-topic, some saturation.';
    else info.textContent = 'Very high guidance — oversaturated colours, reduced diversity.';
  }

  slider.addEventListener('input', update);
  update();
})();


/* ----------------------------------------------------------------
   DEMO 5 — U-NET vs DiT ARCHITECTURE COMPARISON
   ---------------------------------------------------------------- */
(function(){
  const cvU = document.getElementById('unetCanvas');
  const cvD = document.getElementById('ditCompCanvas');
  const cxU = cvU.getContext('2d');
  const cxD = cvD.getContext('2d');
  const stepBtn = document.getElementById('archStepBtn');
  const resetBtn = document.getElementById('archResetBtn');
  const layerDisp = document.getElementById('archLayerDisplay');

  const W = cvU.width, H = cvU.height;
  const G = 8; // 8×8 grid
  const cell = W / G;
  const maxLayers = 6;
  let layer = 0;

  // A "source" pixel that emits information
  const srcR = 1, srcC = 1;

  function drawGrid(ctx, reachSet, colour, label){
    ctx.clearRect(0,0,W,H);
    // background
    for(let r=0;r<G;r++) for(let c=0;c<G;c++){
      const key = r+','+c;
      if(reachSet.has(key)){
        ctx.fillStyle = colour;
      } else {
        ctx.fillStyle = '#f0f0f0';
      }
      ctx.fillRect(c*cell, r*cell, cell, cell);
      ctx.strokeStyle = 'rgba(0,0,0,0.1)';
      ctx.strokeRect(c*cell, r*cell, cell, cell);
    }
    // source highlight
    ctx.strokeStyle = '#EBC043';
    ctx.lineWidth = 3;
    ctx.strokeRect(srcC*cell, srcR*cell, cell, cell);
    ctx.lineWidth = 1;
    // label
    ctx.fillStyle = 'rgba(0,0,0,0.5)';
    ctx.font = '11px sans-serif';
    ctx.fillText('★ source', srcC*cell+3, srcR*cell+cell-4);
  }

  function computeUNetReach(layers){
    // Conv 3×3 receptive field grows by 1 each layer in each direction
    const reach = new Set();
    const radius = layers; // each conv layer expands by 1
    for(let r = srcR-radius; r <= srcR+radius; r++){
      for(let c = srcC-radius; c <= srcC+radius; c++){
        if(r>=0 && r<G && c>=0 && c<G) reach.add(r+','+c);
      }
    }
    return reach;
  }

  function computeDiTReach(layers){
    const reach = new Set();
    if(layers === 0){
      reach.add(srcR+','+srcC);
      return reach;
    }
    // After 1 attention layer: global
    for(let r=0;r<G;r++) for(let c=0;c<G;c++) reach.add(r+','+c);
    return reach;
  }

  function draw(){
    const uReach = computeUNetReach(layer);
    const dReach = computeDiTReach(layer);
    drawGrid(cxU, uReach, 'rgba(59,130,246,0.35)', 'U-Net');
    drawGrid(cxD, dReach, 'rgba(124,58,237,0.35)', 'DiT');
    layerDisp.textContent = 'Layer: ' + layer + ' / ' + maxLayers;
  }

  stepBtn.addEventListener('click', ()=>{
    if(layer < maxLayers) layer++;
    draw();
  });
  resetBtn.addEventListener('click', ()=>{
    layer = 0;
    draw();
  });
  draw();
})();


/* ----------------------------------------------------------------
   DEMO 6 — SCALING CURVE
   ---------------------------------------------------------------- */
(function(){
  const cv = document.getElementById('scalingCanvas');
  const cx = cv.getContext('2d');
  const W = cv.width, H = cv.height;
  const slider = document.getElementById('scaleSlider');
  const valSpan = document.getElementById('scaleVal');
  const fidDisp = document.getElementById('scaleFidDisplay');

  // DiT model points: { name, gflops, fid }
  const models = [
    { name: 'DiT-S/8', gflops: 6, fid: 68 },
    { name: 'DiT-B/4', gflops: 23, fid: 43 },
    { name: 'DiT-L/4', gflops: 80, fid: 23 },
    { name: 'DiT-XL/2', gflops: 119, fid: 9.6 }
  ];

  const pad = { l: 55, r: 25, t: 25, b: 40 };
  const pw = W - pad.l - pad.r;
  const ph = H - pad.t - pad.b;

  function gflopToX(g){ return pad.l + (g / 130) * pw; }
  function fidToY(f){ return pad.t + (f / 80) * ph; }
  // Scaling law: FID ≈ A / gflops^alpha
  function predictFID(g){ return 180 * Math.pow(g, -0.55); }

  function draw(){
    const gVal = parseInt(slider.value);
    valSpan.textContent = gVal;
    const pFID = predictFID(gVal);
    fidDisp.textContent = 'Predicted FID: ' + pFID.toFixed(1) + '  (lower is better)';

    cx.clearRect(0,0,W,H);

    // Axes
    cx.strokeStyle = '#999';
    cx.lineWidth = 1;
    cx.beginPath();
    cx.moveTo(pad.l, pad.t); cx.lineTo(pad.l, H-pad.b); cx.lineTo(W-pad.r, H-pad.b);
    cx.stroke();

    // Labels
    cx.fillStyle = '#666';
    cx.font = '12px sans-serif';
    cx.textAlign = 'center';
    cx.fillText('Model GFLOPs', W/2, H-5);
    cx.save();
    cx.translate(14, H/2);
    cx.rotate(-Math.PI/2);
    cx.fillText('FID (↓ better)', 0, 0);
    cx.restore();

    // Tick marks
    cx.font = '10px monospace';
    cx.textAlign = 'center';
    [0, 20, 40, 60, 80, 100, 120].forEach(g=>{
      const x = gflopToX(g);
      cx.fillStyle = '#999';
      cx.fillText(g, x, H-pad.b+14);
      cx.beginPath(); cx.moveTo(x, H-pad.b); cx.lineTo(x, H-pad.b+4); cx.stroke();
    });
    cx.textAlign = 'right';
    [0, 20, 40, 60, 80].forEach(f=>{
      const y = fidToY(f);
      cx.fillStyle = '#999';
      cx.fillText(f, pad.l-8, y+4);
      cx.beginPath(); cx.moveTo(pad.l-3, y); cx.lineTo(pad.l, y); cx.stroke();
    });

    // Scaling curve
    cx.beginPath();
    cx.strokeStyle = 'rgba(235,192,67,0.6)';
    cx.lineWidth = 2;
    for(let g=2;g<=130;g++){
      const x = gflopToX(g);
      const y = fidToY(Math.min(predictFID(g), 80));
      if(g===2) cx.moveTo(x,y); else cx.lineTo(x,y);
    }
    cx.stroke();

    // Model points
    models.forEach(m=>{
      const x = gflopToX(m.gflops);
      const y = fidToY(m.fid);
      cx.beginPath();
      cx.arc(x, y, 6, 0, Math.PI*2);
      cx.fillStyle = '#EBC043';
      cx.fill();
      cx.strokeStyle = '#b8941e';
      cx.lineWidth = 2;
      cx.stroke();
      cx.fillStyle = '#333';
      cx.font = 'bold 10px sans-serif';
      cx.textAlign = 'left';
      cx.fillText(m.name, x+10, y+4);
    });

    // User slider point
    const ux = gflopToX(gVal);
    const uy = fidToY(Math.min(pFID, 80));
    cx.beginPath();
    cx.arc(ux, uy, 7, 0, Math.PI*2);
    cx.fillStyle = '#7c3aed';
    cx.fill();
    cx.strokeStyle = '#fff';
    cx.lineWidth = 2;
    cx.stroke();
    // dashed line
    cx.setLineDash([4,3]);
    cx.strokeStyle = '#7c3aed';
    cx.lineWidth = 1;
    cx.beginPath(); cx.moveTo(ux, uy); cx.lineTo(ux, H-pad.b); cx.stroke();
    cx.beginPath(); cx.moveTo(pad.l, uy); cx.lineTo(ux, uy); cx.stroke();
    cx.setLineDash([]);
  }

  slider.addEventListener('input', draw);
  draw();
})();


/* ----------------------------------------------------------------
   DEMO 7 — FULL DENOISING PROCESS
   ---------------------------------------------------------------- */
(function(){
  const cvL = document.getElementById('denoiseLatent');
  const cvP = document.getElementById('denoisePred');
  const cvI = document.getElementById('denoiseImage');
  const cxL = cvL.getContext('2d');
  const cxP = cvP.getContext('2d');
  const cxI = cvI.getContext('2d');
  const startBtn = document.getElementById('denoiseStartBtn');
  const resetBtn = document.getElementById('denoiseResetBtn');
  const stepDisp = document.getElementById('denoiseStep');
  const speedSlider = document.getElementById('denoiseSpeed');
  const speedVal = document.getElementById('denoiseSpeedVal');

  const SZ = cvL.width;
  const totalSteps = 50;
  let step = 0;
  let running = false;
  let animId = null;

  // Target image: a simple scene
  function targetPixel(x, y){
    const nx = x/SZ, ny = y/SZ;
    // Sky gradient
    if(ny < 0.45){
      const t = ny / 0.45;
      return [135-t*30, 195-t*20, 235-t*10];
    }
    // Sun
    const sdx = nx-0.8, sdy = ny-0.2;
    if(sdx*sdx+sdy*sdy < 0.015) return [255, 220, 60];
    // Mountains
    const mh = 0.45 + 0.08*Math.sin(nx*12) + 0.05*Math.cos(nx*20);
    if(ny < mh + 0.06) return [100, 80, 120];
    // Grass
    const gh = 0.65 + 0.02*Math.sin(nx*30);
    if(ny < gh) return [50+nx*30, 140+ny*20, 50];
    // Flowers
    if(Math.sin(x*1.5)*Math.cos(y*1.3) > 0.9) return [230, 90, 90];
    return [40+ny*40, 130+nx*20, 40];
  }

  // Precompute target
  const target = [];
  for(let y=0;y<SZ;y++) for(let x=0;x<SZ;x++) target.push(targetPixel(x,y));

  // Noise field (fixed)
  const rng = mulberry32(123);
  const noise = [];
  for(let i=0;i<SZ*SZ;i++) noise.push([rng()*255, rng()*255, rng()*255]);

  function drawCanvas(ctx, data){
    const img = ctx.createImageData(SZ, SZ);
    for(let i=0;i<data.length;i++){
      img.data[i*4]   = Math.max(0,Math.min(255,data[i][0]));
      img.data[i*4+1] = Math.max(0,Math.min(255,data[i][1]));
      img.data[i*4+2] = Math.max(0,Math.min(255,data[i][2]));
      img.data[i*4+3] = 255;
    }
    ctx.putImageData(img, 0, 0);
  }

  function render(){
    const t = 1 - step / totalSteps; // noise fraction remaining (1=pure noise, 0=clean)
    // Current latent: interpolation between noise and target
    const latent = [];
    const pred = [];
    for(let i=0;i<SZ*SZ;i++){
      latent.push([
        target[i][0]*(1-t) + noise[i][0]*t,
        target[i][1]*(1-t) + noise[i][1]*t,
        target[i][2]*(1-t) + noise[i][2]*t
      ]);
      // Predicted noise = current - target (scaled)
      pred.push([
        128 + (noise[i][0]-128)*t,
        128 + (noise[i][1]-128)*t,
        128 + (noise[i][2]-128)*t
      ]);
    }
    drawCanvas(cxL, latent);
    drawCanvas(cxP, pred);
    // Decoded image: the "clean" estimate at this step
    const decoded = [];
    for(let i=0;i<SZ*SZ;i++){
      // At current step, our best estimate is the latent minus predicted noise
      const cleanEst = 1 - t;
      decoded.push([
        target[i][0]*cleanEst + 128*(1-cleanEst),
        target[i][1]*cleanEst + 128*(1-cleanEst),
        target[i][2]*cleanEst + 128*(1-cleanEst)
      ]);
    }
    drawCanvas(cxI, decoded);
    stepDisp.textContent = 'Step: ' + step + ' / ' + totalSteps;
  }

  function tick(){
    if(!running || step >= totalSteps){ running=false; startBtn.textContent='▶ Start Denoising'; return; }
    step++;
    render();
    const speed = parseInt(speedSlider.value);
    const delay = Math.max(20, 200 - speed*18);
    animId = setTimeout(tick, delay);
  }

  startBtn.addEventListener('click', ()=>{
    if(running){ running=false; startBtn.textContent='▶ Continue'; clearTimeout(animId); return; }
    if(step >= totalSteps){ step=0; }
    running=true;
    startBtn.textContent='⏸ Pause';
    tick();
  });

  resetBtn.addEventListener('click', ()=>{
    running=false; clearTimeout(animId);
    step=0;
    startBtn.textContent='▶ Start Denoising';
    render();
  });

  speedSlider.addEventListener('input', ()=>{ speedVal.textContent = speedSlider.value; });

  render();
})();

</script>